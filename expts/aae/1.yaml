encoder: 
    conv_before:
      - kernel_size: 64
        activation: 'relu'
        b_norm: False
      - kernel_size: 64
        activation: 'relu'
        b_norm: False
    conv_after:
      - kernel_size: 128
        activation: 'relu'
        b_norm: False
    fc_layers:
      - out_size: 64
        activation: 'relu'
        b_norm: False
    hpool: 
      arch: ~
    gpool:
      arch: 'sum'
    edge_layer:
        depth: 3
        pooling: 'avg'

decoder:
  layers:
    - out_size: 128
      activation: 'relu'
      b_norm: False
    - out_size: 128
      activation: 'relu'
      b_norm: False
  node_layers:
    - out_size: 64
      activation: 'relu'
      b_norm: False 
  node_layers:
    - out_size: 64
      activation: 'relu'
      b_norm: False
  graph_layers:
    - out_size: 128
      activation: 'relu'
      b_norm: False
  graph_layers:
    - out_size: 128
      activation: 'relu'
      b_norm: False

discriminator:
  layers_dim:
    - 64
    - 32
  activation: 'relu'
  b_norm: True


latent_dim: 128